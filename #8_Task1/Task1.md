GitHub地址：https://github.com/f1shungry/Jotang-recruit



中途换过一次电脑，不小心把做Task1时写的笔记弄丢了T T当时的截图也找不到了

这个md文件是重新写过的





### 准备



#### 安装anaconda

都说最好用anaconda搭建虚拟环境，于是我就按照网上找的的帖子中的步骤，麻木地点击一个个按钮，在``虚拟环境到底是什么该怎么用，conda又是什么该怎么用``都不清楚的情况下，把anaconda安装好了。

那时的我，还不知道，在接下来的三十多天里，将受到来自环境配置的非人的折磨……

我似懂非懂地打开cmd，用命令 ``conda create -n pytorch python=3.7``	``activate pytorch`` 两个命令创建了一个叫 pytorch 的虚拟环境，再输入 ``jupyter notebook`` 打开jupyter notebook，懵懵懂懂地开始了ML之旅



#### 学习Python基本语法

目标是能大致看懂后续题目的代码，暂时不求精通Python。用的是《Python编程：从入门到实践》这本书，学习到第九章 *类*

学习过程中也是写了笔记的，也搞丢了T T

学习以后的一些感受：

- Python中没有花括号，语句结尾不要分号。同时对缩进的要求很严格，for循环、if-else语句后要跟分号（这个很容易忘）
- 跟Java不同的一点是Python的类都有 __ init __这个方法，其中必须包括一个形参self。 



#### 《python神经网络编程》第一章

从含有一个隐藏层，每层三个神经元的简单神经网络起步， **大致了解了神经网络的构成**， **向前传播和反向传播过程可以用矩阵乘法来实现**， 让繁琐的计算得到简化。



#### 了解机器学习



##### 监督学习/无监督学习

以机器学习中的分类(classification)来说，输入的训练数据有特征（feature），有标签（label）。在分类过程中，如果所有训练数据都有标签，则为有监督学习（supervised learning）。如果数据没有标签，显然就是无监督学习（unsupervised learning）。

监督学习，就是通过已有的训练样本（即已知数据以及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优则表示在某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出，对输出进行简单的判断从而实现分类的目的，也就具有了对未知数据进行分类的能力。 典型的例子就是KNN、SVM。

无监督学习中，输入的样本数据没有标签，需要直接对数据进行建模，分析出数据的结构。无监督学习里典型的例子就是聚类了。



##### 分类器/预测器

分类器是在已有数据基础上构造出一个分类模型，该模型能够把数据库中的数据记录映射到给定类别中的某一个，从而应用于数据预测。

预测器：接收一个简单的输入，做出应有的预测，再输出结果



##### 独热码

One-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。

One-Hot编码是分类变量作为二进制向量的表示。这首先要求将分类值映射到整数值。然后，每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。



举个例子，假设有一群学生，他们可以通过四个特征来形容，分别是：

- 性别：[“男”，“女”]
- 年级：[“初一”，“初二”，“初三”]
- 学校：[“一中”，“二中”，“三中”，“四中”]

用上述四个特征来描述小明同学，即“男生，初一，来自二中”，如果特征类别是有序的话，我们能够用表示顺序的数组表示

即“男生，初一，来自一中”   ==>   [0,0,1]

但是这样的特征处理并不能直接放入机器学习算法中，因为类别之间是无序的。

这时候就可以用独热编码的形式来表示了，我们用采用N位状态寄存器来对N个状态进行编码，拿上面的例子来说，就是：

| 性别 | [“男”，“女”]                     | N=2  | 男：1 0女：0 1                                       |
| :--- | :------------------------------- | :--- | :--------------------------------------------------- |
| 年级 | [“初一”，“初二”，“初三”]         | N=3  | 初一：1 0 0  初二：0 1 0初三：0 0 1                  |
| 学校 | [“一中”，“二中”，“三中”，“四中”] | N=4  | 一中：1 0 0 0二中：0 1 0 0三中：0 0 1 0四中：0 0 0 1 |

因此，当我们再来描述小明的时候，就可以采用 [1 0 1 0 0 0 1 0 0] 

**One-Hot编码的作用**

之所以使用One-Hot编码，是因为在很多机器学习任务中，特征并不总是连续值，也有可能是离散值（如上表中的数据）。将这些数据用数字来表示，执行的效率会高很多。

- 性别：[“男”，“女”]
- 年级：[“初一”，“初二”，“初三”]
- 学校：[“一中”，“二中”，“三中”，“四中”]

若是直接转换成数字的话，[“男”，“初二”，“四中”]的表示方式就是[0,1,3]。

然而，即使转化为数字表示后，上述数据也不能直接用在分类器中。因为分类器往往默认数据数据是连续的、有序的。但是，直接数字并不是有序的，而是随机分配的。为了解决上述问题，其中一种可能的解决方法是采用独热编码。



##### 向量化

我对向量化的理解就是把需要相乘并相加的数据整理成两个矩阵，用矩阵乘法实现相乘再相加的操作，这样运行起来很快



##### 神经元/输入层/隐藏层/输出层/卷积

- 神经网络中单个结点称为神经元。输入层是接受输入的数据，隐藏层在输入层之后，可以是一层，也可以是两层或多层。隐藏层会对从前一层接受到的数据做一些计算，比如乘上自带的权重，再经过非线性函数的映射，把新的数据传递到下一层。输出层就是接收最终得到的数据。

- 卷积的定义，看得我头晕……只能大致做个了解

  可以把卷积想象成一种混合信息的手段。想象一下装满信息的两个桶，我们把它们倒入一个桶中并且通过某种规则搅拌搅拌。也就是说卷积是一种混合两种信息的流程。

  卷积也可以形式化地描述，事实上，它就是一种数学运算，跟减加乘除没有本质的区别。虽然这种运算本身很复杂，但它非常有助于简化更复杂的表达式



##### 激活函数/权重/权重更新

- 激活函数

在神经网络中，输入数据经过加权求和后，可能需要经过一个函数的映射，再传递到下一层，这个函数就是激活函数。

- 为什么要使用激活函数？

神经网络中每一层的输入输出都是一个线性求和的过程，下一层的输出只是承接了上一层输入函数的线性变换。引入激活函数后（通常都是非线性的），会给神经元引入非线性元素，使神经网络应用到更多非线性模型中



- 权重 / 权重更新

权重的大小代表了神经网络中对特征的放大程度。权重更新就是模型训练的重要过程，一般来说训练模型就是为了得到最优的权重组合



##### 梯度/学习率/损失函数/过拟合

- 梯度：

梯度的定义：

![梯度定义](https://img-blog.csdn.net/20160325132321423)

它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。

函数沿梯度方向具有最大的变化率，那么我们沿着**负梯度方向**减小损失函数值，就可以优化模型



- 学习率：在梯度下降的过程中更新权重时的超参数。当学习率过大的时候会导致模型难以收敛，过小的时候会收敛速度过慢



- 损失函数：损失函数是用来估量 **模型的输出 **与 **真实值** 之间的差距，给模型的优化指引方向。



- 过拟合

就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测/区分了所有的数据，但是在新的测试集上却表现平平，不具泛化性，拿到新样本后没有办法去准确的判断。

造成过度拟合的原因可能有多种，最常见的就是模型容量过高，模型过于复杂，换句话说是模型假设所包含的参数数量过多。如此一来，算法会将训练集中所包含的没有普遍性的一些特征也学习进来，结果降低了模型的泛化能力。

我觉得这样一个例子比较形象：

> 打个可能不太恰当的比方，一个饱经沧桑，经历过各种复杂人际关系的人在遇到一个心思纯粹的人时，容易将对方想得很复杂，反而难以理解对方。

> 提高模型泛化能力背后的哲学思想正是所谓的 “奥卡姆剃刀” 原理：在能够解释所观察到的现象的各种不同理论中，我们尽可能去选择那个最简单的理论。





##### 训练集/测试集/神经网络性能评价指标

- 训练集是指用来训练模型的输入数据的集合。测试集是用来测试模型性能的数据的集合。



- 神经网络性能评价指标

首先，所有的样本（数据）中，可以被分为正样本和负样本。

![img](https://f1sh-hungry.oss-cn-chengdu.aliyuncs.com/f77683adfd9f14ab8887d41788755de3.png)

其次，我们在对样本进行分类的时候也会分出正样本和负样本，但我们的判断是有错误的，故存在以下情况：

True Positive(真正，**TP**)：将正样本预测为正样本

True Negative(真负，**TN**)：将负样本预测为负样本

False Positive(假正，**FP**)：将负样本预测为正样本

False Negative(假负，**FN**)：将正样本预测为负样本

所以，P为正样本，N为负样本，就有以下关系：
$$
TP + FN = P \quad \quad TN + FP = N
$$


根据混淆矩阵提供的概念，我们可以延伸出准确率（Accuracy）， 错误率（Error rate）， 灵敏度（sensitive） ，特效度（sensitive） ， 精确率、精度（Precision）， 召回率（recall） ，综合评价指标（F-Measure） 。这些指标是评价网络的基础要素。

1、**准确率（Accuracy）**
首先，它的计算公式：
$$
ACC = \frac{TP+TN}{P+N}
$$


直白的看，意思就是在所有样本中，我们判断对了多少。 显然是ACC越高越好（保持其他变量不变的话）。

但是！ACC很高（100%除外）并不一定代表网络优秀。

打个比方：有10w个样本，其中9.999w都是正样本，剩下的只有10个负样本，你的网络只要在正样本中表现的好就能达到很高的准确率。所以那10个负样本的重要性被忽略了，如果那10个负样本才是重心的话，这个准确率就没有什么现实意义。（其中也暴露出另一个问题，数据的分布对网络的训练和评价也是非常重要的！）

2、**错误率（Error rate）**
首先，它的计算公式：
$$
Er = \frac{FP+FN}{P+N}
$$
也就是：
$$
1 − A C C = E r 
$$


3、**灵敏度（Sensitive) 特效度（Specificity）**
灵敏度（也叫召回率）的计算公式：
$$
Sensitive = \frac{TP}{P}
$$
特效度（特异度）的计算公式：
$$
Specificity = \frac{TN}{N}
$$


如计算公式所示，灵敏度和特效度分别是所有正样本中被判断正确的比例 和 所有负样本中被判断正确的比例，分别衡量了分类器对正负样本的识别能力。



4、**精确率、精度（Precision）**
公式：
$$
Precision = \frac{TP}{TP+FP}
$$


表示被分为正样本的数据中实际为正样本的比例。

5、综合评价指标（F-Measure）
精度和灵敏度有时候会出现矛盾的情况，所以需要被综合考虑，常用的有F-Measure（又称为F-Score）

F-Measure为Precision和Sensitive的加权平均：
$$
F = \frac{(\alpha^2+1)*P*R}{\alpha^2(P+R)}
$$
当α=1的时候，就是F1指标：
$$
F1 = \frac{2*P*R}{P+R}
$$


可见F-Measure的计算调和了Precision和Sensitive，且F-Measure的值越高说明网络性能越强

现在来看一下另一个常见的指标

6、**ROC曲线**
ROC（Receiver Operating Characteristic）曲线是以假正率（FP_rate）和假负率（TP_rate）为轴的曲线。

通常我们称ROC曲线下面积为AUC，如图所示：、![在这里插入图片描述](https://f1sh-hungry.oss-cn-chengdu.aliyuncs.com/20210713194027539.png)



TP_rate指的就是灵敏度（相对无脑来说，越高越好）
$$
 TPrate = \frac{TP}{P}
$$




FP_rate指的是负样本中误判断为正样本的概率（相对无脑来说，越低越好）
$$
FPrate = \frac{FP}{N}
$$


所以A的性能最好，B的性能最差。即：曲线越靠近A点（左上方）性能越好，曲线越靠近B点（右下方）曲线性能越差。

也可以判断出，L2的性能比L1要好。

其中，对角线CD的数学含义，指TP_rate==FP_rate，实际含义就是五五开，瞎猜的概率。（由此可知，对角线之下的点，就表明网络的性能很差）





### 启航



#### 完善iris.py

想要让做题来驱动学习，所以现在就准备完善iris.py，遇到不懂的就去学

首先是发现iris.py能打开但不能编辑，只好新建一个notebook把iris.py的内容复制粘贴上去

后来才知道，notebook中的文件后缀是ipynb，跟py文件是两回事~

打开文件以后：哈哈哈完全看不懂

于是开始大量的查阅资料，了解每个库是干嘛的，relu，linear，softmax该怎么用等等



###### sklearn

```
from sklearn import datasets
dataset = datasets.load_iris()
input, x_test, label, y_test = train_test_split( dataset['data'],dataset['target'],test_size=0.8,random_state=0 )
```

sklearn 全名为 scikit-learn ，是机器学习中一个库，其中包括一些经典数据集。上方第二行代码就是从这个库中导出iris数据集。train_test_split函数可以把数据集划分为训练集和测试集。题目要求按二八比例划分，那么把test_size设置为0.8即可（test集占总数据集的比例）

注意sklearn库的安装：要输入它的全名 **scikit-learn**，否则无法安装！



###### 张量（Tensor）

查了资料以后，对张量似懂非懂，我当前的理解就是**多维数组**



###### epoch

机器学习中常用epoch这个单词指代要训练的次数



###### 反向传播

我本来以为反向传播的过程需要自己写，但看了这个程序才知道pytorch的backward函数可以自己计算并完成反向传播过程



#### 配置环境

完善了iris.py后，把需要的库都安装好，然后尝试ctrl+Enter运行。报错弹出的那一刻，开启了人生第一次配置环境的艰苦过程

配置环境的过程遇到了很多错误，我在电脑面前蹲了一天多，才把问题解决。

我做的记录很少，这里只能概述一下问题所在：

- numpy 版本和 pytorch 不适配。重新安装的过程也遇到了网络的问题，并且安装速度有点慢，花了很长时间
- jupyter notebook 的使用方法没有搞懂。我一直以为，在cmd中activate 虚拟环境，再输入jupyter notebook并按下Enter键进入 jupyter notebook 网页以后，jupyter notebook 就进入了创建的虚拟环境了。但其实我这样做，jupyter notebook 使用的是base环境，而我安装库、卸载库的操作都在虚拟环境中进行的，这就是我一直跑不起来代码的原因。

- 正确的做法：
  - 每创建并激活一个虚拟环境后，``conda install nb_conda``安装nb插件，再``conda install -y jupyter ``

在该虚拟环境中启用jupyter notebook。这样才能在 jupyter notebook 中切换虚拟环境 



有一点后悔没有做好记录，但没关系，后面几题有的是配置环境的艰难困苦：)



#### 运行结果

![](https://f1sh-hungry.oss-cn-chengdu.aliyuncs.com/QQ%E6%88%AA%E5%9B%BE20220915115522.png)

